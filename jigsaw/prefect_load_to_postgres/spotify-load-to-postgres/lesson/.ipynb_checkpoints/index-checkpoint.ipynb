{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b975534-11c6-4b51-9593-2d5475e21afc",
   "metadata": {},
   "source": [
    "# Load to Postgres Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b121b2a-d523-44ad-92de-1e02cc7ece8e",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f21db-2df8-4fa8-8ee4-bc1a5641e64a",
   "metadata": {},
   "source": [
    "In this lesson, we'll work with prefect deployments to schedule requests to the Spotify API.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64a7b4-ed1a-443d-9744-46d1131ef458",
   "metadata": {},
   "source": [
    "### Getting set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59145317-050a-4097-99f4-ea64b82dd43c",
   "metadata": {},
   "source": [
    "You'll notice that we are again working with the spotify codebase from the previous lab. \n",
    "\n",
    "* Do **not** create a python environment for this lab -- it will likely mess things up.  In fact, make sure there is no python environment activated\n",
    "```bash\n",
    "    deactivate\n",
    "    conda deactivate\n",
    "```\n",
    "\n",
    "* Install the libraries in the `requirements.txt` file.\n",
    "\n",
    "* Add in your spotify API keys to the `etl/spotify_extractor/.env` file.  If you do not have spotify api keys, you can see the docs on how to create them [here](https://developer.spotify.com/documentation/web-api/tutorials/getting-started). \n",
    "\n",
    "* From there run `python3 spotify_workflow.py`, which will run the workflow.  Confirm that the track data is saved in the `data` folder.  \n",
    "\n",
    "> Please look at the file itself, which should look like the following:\n",
    "```csv\n",
    ",track_id,ranking,date,playlist_id\n",
    "0,4xhsWYTOGcal8zt0J161CU,1,2024-01-11,37i9dQZEVXbLRQDuF5jeBp\n",
    "1,0mflMxspEfB0VbI1kyLiAv,2,2024-01-11,37i9dQZEVXbLRQDuF5jeBp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d705e6-dc09-431e-9cee-4019b1174e26",
   "metadata": {},
   "source": [
    "### Our new workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c2222-c411-4390-9567-540e057b70ab",
   "metadata": {},
   "source": [
    "So in this lesson, we will add to our workflow so that it will look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef333bd-8e54-4c55-83c2-1bbdf524f17a",
   "metadata": {},
   "source": [
    "<img src=\"./load-to-pg-workflow.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1627dc1-a0bd-412e-9b0e-c5cc7bb217b2",
   "metadata": {},
   "source": [
    "In other words, this time we will pull data from the API, coerce the data and then write to a csv file.  And from there, we will read from that CSV file (really a set of csv files), and load our recent data to the postgres database.\n",
    "\n",
    "So writing to the CSV acts as a staging layer.\n",
    "\n",
    "> **Why staging?**  Staging is a typical component in a data pipeline.  It's main purpose is that it allows us to save data that is often less transformed (or not transformed at all), before then coercing the data to load to a database.  This is helpful, because whenever we transform data, we are inevitably losing some data -- we no longer have the original.  So by saving our data in a more raw form, we are holding onto more data.  And this is data, that is probably *too raw* to store in our database (both because the data is too messy to query day to day, and because it's more costly to store low value data in a database than in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ffc63-1d89-45d1-be45-faabf4f2ea36",
   "metadata": {},
   "source": [
    "### Loading to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ae355-10f6-4fab-81a3-d0e7069048e1",
   "metadata": {},
   "source": [
    "* Create the database\n",
    "\n",
    "    * Create a database in your local postgres instance called `spotify` top songs\n",
    "    * Then run the file in the migrations folder to create a new table in the database\n",
    "    \n",
    "* Connect to the database\n",
    "    * If you look in the db.py file, you can see that we have added some that uses the sqlalchemy library to connect to the database.\n",
    "\n",
    "* From there, add the following function to the `listings_adapter.py` file\n",
    "```python\n",
    "    def load_to_postgres(df, engine, table_name = 'tracks'):\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "```\n",
    "The function, takes a dataframe, and uses the `to_sql` method to save it to the specified table (tracks).\n",
    "\n",
    "Make sure that the function works properly by running the corresponding code in the console, and then connecting to postgres to confirm that the data has been written.\n",
    "\n",
    "* You can also see the `read_sql` function\n",
    "    * This allows us to use pandas to read data from our postgres database\n",
    "```python\n",
    "    from sqlalchemy import text\n",
    "    def read_sql(query, engine):\n",
    "        df = pd.DataFrame(engine.connect().execute(text(query)))\n",
    "        return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b96ce-ca1f-4502-aba3-a7f7b109f575",
   "metadata": {},
   "source": [
    "```sql\n",
    "spotify=# select * from tracks limit 3;\n",
    "        track_id        | ranking |    date    |      playlist_id\n",
    "------------------------+---------+------------+------------------------\n",
    " 7gaA3wERFkFkgivjwbSvkG |       1 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    " 52eIcoLUM25zbQupAZYoFh |       2 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    " 4xhsWYTOGcal8zt0J161CU |       3 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f148324-36fb-4620-bbc8-c470c64efc7c",
   "metadata": {},
   "source": [
    "### Revisiting our process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea570c-3c89-44b8-9a18-12605c0cd7c4",
   "metadata": {},
   "source": [
    "So now we have code to pull from the Spotify API, and coerce the data and then save it to a csv, from there we can read the data from the csv into a dataframe, and from there into the postgres database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81b1ee-c239-4ebf-82fe-e506c65701f1",
   "metadata": {},
   "source": [
    "Now, eventually we'll schedule this to run every day.  And this makes sense as the playlist is for the [top fifty songs](https://open.spotify.com/playlist/37i9dQZEVXbLRQDuF5jeBp) in the USA. Which will change each day.\n",
    "\n",
    "But we do not just want to just keep appending to the same CSV file.  So instead let's update the write CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a8fea-a789-4416-9ac0-bbb522d50a68",
   "metadata": {},
   "source": [
    "* Update `write_to_csv`\n",
    "    * Now instead of writing to a file `./data/track_listings.csv`, update the `write_to_csv` function so that the date is included in the file name.  For example, if the date is Jan 17 2024 the track listings file path should be `./data/2024-01-17-track_listings.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e092c42-e975-458e-801c-8aeae8e4558d",
   "metadata": {},
   "source": [
    "* `find_recent_files`\n",
    "    * So now we can `write_to_csv`, and then load the csv files into the database.  However, we want to make sure we are not re-inserting the same data into the database.  To prevent against this, write a function that only finds files later than the most recently added data:\n",
    "        * First use the `read_sql` function to find for the most recent date that we loaded data into the database.\n",
    "        * Then only return the file names after that date.\n",
    "        * For example, you can see that we have included files from 2024, and 2025.  The function should return a list of files with the file from 2025. \n",
    "\n",
    "* `load_files_to_postgres`\n",
    "    * The `load_files_to_postgres` calls the `find_recent_files` function, and then reads those csv files, and loads them to a database. \n",
    "    * You can see in the `2025` csv file that we added a track `sample-data` that should be loaded into postgres  after calling the `load_files_to_postgres` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9c397-691e-4dbe-8f83-c6d225c3b46b",
   "metadata": {},
   "source": [
    "### Updating the flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b9c1b-49c2-45bd-bd94-d634bb4d79bf",
   "metadata": {},
   "source": [
    "Now in the `spotify_workflow.py` file, add the following:\n",
    "    \n",
    "1. New `load_files_to_postgres` task\n",
    "    * This should call the function in the adapter\n",
    "    \n",
    "2. Add the task to the flow\n",
    "    * Add this `load_files_to_postgres` task to the flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee48f26-f7a4-4603-9f14-fc35e87cfb0a",
   "metadata": {},
   "source": [
    "Run the `spotify_workflow.py` file, and confirm that the data is loaded to postgres (there will be a new record with `sample-data` as the track each time you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47ed95-1a93-43fc-a9f4-dd0994e19851",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resources\n",
    "\n",
    "[Jeff Hale - Deployments](https://medium.com/the-prefect-blog/deploy-prefect-pipelines-with-python-perfect-68c944a3a89f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc250cab-da27-4576-8856-d9c5a84ae9d0",
   "metadata": {},
   "source": [
    "[Kevin Kho](https://medium.com/the-prefect-blog/the-simple-guide-to-productionizing-data-workflows-with-docker-31a5aae67c0a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a4e9f-246b-44d1-907f-bc4c527594e0",
   "metadata": {},
   "source": [
    "[Creating a Deployment](https://discourse.prefect.io/t/error-when-creating-a-deployment-with-the-cli-modulenotfound/2426/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd874f9a-9f77-47db-8d65-cf36d0af851c",
   "metadata": {},
   "source": [
    "[Sample deployment](https://github.com/PrefectHQ/prefect/issues/8710)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
